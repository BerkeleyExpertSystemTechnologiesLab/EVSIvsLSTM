{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sampled_train = pd.read_csv(\"dataset/train.csv\")\n",
    "Sampled_test = pd.read_csv(\"dataset/test.csv\")\n",
    "Sampled_cv = pd.read_csv('dataset/cv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some mysterious fault type\n",
    "Sampled_train.drop(Sampled_train[(Sampled_train.faultNumber == 3) | (Sampled_train.faultNumber == 9) | (Sampled_train.faultNumber == 15)].index, inplace = True)\n",
    "Sampled_test.drop(Sampled_test[(Sampled_test.faultNumber == 3) | (Sampled_test.faultNumber == 9) | (Sampled_test.faultNumber == 15)].index, inplace = True)\n",
    "Sampled_cv.drop(Sampled_cv[(Sampled_cv.faultNumber == 3) | (Sampled_cv.faultNumber == 9) | (Sampled_cv.faultNumber == 15)].index, inplace = True)\n",
    "\n",
    "# make the Y value usable in LSTM\n",
    "y_train = to_categorical(Sampled_train['faultNumber'],num_classes=21)\n",
    "y_test = to_categorical(Sampled_test['faultNumber'],num_classes=21)\n",
    "y_cv = to_categorical(Sampled_cv['faultNumber'],num_classes=21)\n",
    "\n",
    "# drop unused meta data from x\n",
    "x_train_df = Sampled_train.drop(['faultNumber','simulationRun','sample'],axis=1)\n",
    "x_test_df = Sampled_test.drop(['faultNumber','simulationRun','sample'],axis =1)\n",
    "x_cv_df = Sampled_cv.drop(['faultNumber','simulationRun','sample'],axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_false_positive = 2\n",
    "cost_false_negative = 16\n",
    "\n",
    "# ['xmv_10', 'xmv_11', 'xmeas_19', 'xmeas_21', 'xmv_9', 'xmv_4', 'xmv_5', 'xmeas_17', 'xmeas_18', 'xmeas_9']\n",
    "\n",
    "sensors = ['xmeas_19', 'xmv_9', 'xmv_4']\n",
    "initial_model_path = 'models/evsi/' + '7_' + \"xmv_11/\"\n",
    "evsi_counter = 7\n",
    "\n",
    "# get prior probability\n",
    "temp = Sampled_train['faultNumber'].value_counts()\n",
    "non_fault = temp[0]\n",
    "total = temp.sum()\n",
    "\n",
    "temp = Sampled_cv['faultNumber'].value_counts()\n",
    "non_fault += temp[0]\n",
    "total += temp.sum()\n",
    "\n",
    "P_present = non_fault/total\n",
    "P_absent = 1 - P_present\n",
    "\n",
    "# ground truth\n",
    "ground_truth = Sampled_test['faultNumber'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_remover(features_names):\n",
    "    # remove a list of features from x\n",
    "    \n",
    "    dimension = dict()\n",
    "    \n",
    "    # row dimension\n",
    "    dimension['train_row'] = len(x_train_df)\n",
    "    dimension['test_row'] = len(x_test_df)\n",
    "    dimension['cv_row'] = len(x_cv_df)\n",
    "    \n",
    "    # create a copy so we don't change the original dataframe\n",
    "    x_train_masked_df = x_train_df.copy()\n",
    "    x_test_masked_df = x_test_df.copy()\n",
    "    x_cv_masked_df = x_cv_df.copy()\n",
    "    \n",
    "    for feature in features_names:\n",
    "        x_train_masked_df.drop([feature], axis = 1, inplace = True)\n",
    "        x_test_masked_df.drop([feature], axis = 1, inplace = True)\n",
    "        x_cv_masked_df.drop([feature], axis = 1, inplace = True)\n",
    "        \n",
    "    # column dimension\n",
    "    dimension['train_col'] = x_train_masked_df.shape[1]\n",
    "    dimension['test_col'] = x_test_masked_df.shape[1]\n",
    "    dimension['cv_col'] = x_cv_masked_df.shape[1]\n",
    "    \n",
    "    standard_scalar = StandardScaler()\n",
    "    x_train_masked_df = standard_scalar.fit_transform(x_train_masked_df)\n",
    "    x_test_masked_df = standard_scalar.transform(x_test_masked_df)\n",
    "    x_cv_masked_df = standard_scalar.transform(x_cv_masked_df)\n",
    "    \n",
    "    x_train = np.resize(x_train_masked_df, (dimension['train_row'], dimension['train_col'], 1))\n",
    "    x_test = np.resize(x_test_masked_df, (dimension['test_row'], dimension['test_col'], 1))\n",
    "    x_cv = np.resize(x_cv_masked_df, (dimension['cv_row'], dimension['cv_col'], 1))\n",
    "    \n",
    "    return dimension, x_train, x_test, x_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, x_cv, y_cv, train_col, feature_name, model_path):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256,input_shape= (train_col, 1),return_sequences= True))\n",
    "    model.add(LSTM(128,return_sequences= False))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(21,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model_path + feature_name)\n",
    "    print(train_col)\n",
    "    \n",
    "    # training\n",
    "    model.fit(x_train, y_train, epochs=35,verbose=0,batch_size=256,validation_data = (x_cv, y_cv))\n",
    "    \n",
    "    # saving the model\n",
    "    model.save(model_path + feature_name)\n",
    "    \n",
    "    # saving the history\n",
    "    model_paras = model.history\n",
    "    with open(model_path + feature_name + '/history.pickle', 'wb') as handle:\n",
    "        pickle.dump(model_paras.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to calculate probability of correctly giving signal when present\n",
    "def get_signal_present(prediction, ground_truth):\n",
    "    present_index = list()\n",
    "    for i in range(len(ground_truth)):\n",
    "        if ground_truth[i] == 0:\n",
    "            present_index.append(i)\n",
    "    \n",
    "    counter = 0\n",
    "    for index in present_index:\n",
    "        if prediction[index] == 0:\n",
    "            counter += 1\n",
    "    \n",
    "    return counter/len(present_index)\n",
    "\n",
    "# helper function to calculate probability of correctly giving signal when present\n",
    "# there should be a more generic way using operator module to merge this with the one above.\n",
    "def get_no_signal_absent(prediction, ground_truth):\n",
    "    absent_index = list()\n",
    "    for i in range(len(ground_truth)):\n",
    "        if ground_truth[i] != 0:\n",
    "            absent_index.append(i)\n",
    "    \n",
    "    counter = 0\n",
    "    for index in absent_index:\n",
    "        if prediction[index] != 0:\n",
    "            counter += 1\n",
    "    return counter/len(absent_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_cost(prediction, ground_truth):\n",
    "  # get P(signal|present) and P(no signal|absent)\n",
    "    P_signal_present = get_signal_present(prediction, ground_truth)\n",
    "    P_no_signal_absent = get_no_signal_absent(prediction, ground_truth)\n",
    "    P_signal_absent = 1 - P_no_signal_absent\n",
    "    P_no_signal_present = 1 - P_signal_present\n",
    "\n",
    "  # get P(signal)\n",
    "    P_signal = P_present * P_signal_present + P_absent * P_signal_absent\n",
    "    P_no_signal = 1 - P_signal\n",
    "\n",
    "  # bayesian probability\n",
    "    P_absent_signal = (P_signal_absent * P_absent) / P_signal\n",
    "    P_present_signal = (P_signal_present * P_present) / P_signal\n",
    "    P_absent_no_signal = (P_no_signal_absent * P_absent) / P_no_signal\n",
    "    P_present_no_signal = (P_no_signal_present * P_present) / P_no_signal\n",
    "\n",
    "  #calculate the expected cost\n",
    "    cost = P_signal * min(cost_false_positive * P_absent_signal, cost_false_negative * P_present_signal) + P_no_signal * min(cost_false_positive * P_absent_no_signal, cost_false_negative * P_present_no_signal)\n",
    "  \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dict(dictionary):\n",
    "    x, y = [], []\n",
    "    for key, value in dictionary.items():\n",
    "        x.append(key)\n",
    "        y.append(value)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Calculating EVSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/evsi/7_xmv_11/base\n",
      "49\n",
      "WARNING:tensorflow:From D:\\Enviroment\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/evsi/7_xmv_11/base\\assets\n",
      "WARNING:tensorflow:From <ipython-input-10-136fcdd76aff>:17: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "models/evsi/7_xmv_11/+xmeas_19\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/evsi/7_xmv_11/+xmeas_19\\assets\n",
      "models/evsi/7_xmv_11/+xmv_9\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/evsi/7_xmv_11/+xmv_9\\assets\n",
      "models/evsi/7_xmv_11/+xmv_4\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/evsi/7_xmv_11/+xmv_4\\assets\n",
      "models/evsi/8_xmeas_19/base\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/evsi/8_xmeas_19/base\\assets\n",
      "models/evsi/8_xmeas_19/+xmv_9\n",
      "51\n",
      "INFO:tensorflow:Assets written to: models/evsi/8_xmeas_19/+xmv_9\\assets\n",
      "models/evsi/8_xmeas_19/+xmv_4\n",
      "51\n",
      "INFO:tensorflow:Assets written to: models/evsi/8_xmeas_19/+xmv_4\\assets\n"
     ]
    }
   ],
   "source": [
    "model_path = initial_model_path\n",
    "unused_sensor = list()\n",
    "\n",
    "#write header\n",
    "# with open('log/sensor_selection.csv', 'a', newline='') as out:\n",
    "#     csv_out = csv.writer(out)\n",
    "#     csv_out.writerow(['sensor', 'evsi'])\n",
    "\n",
    "while len(sensors) > 1:\n",
    "    unused_sensor.append(sensors.copy())\n",
    "    #print(unused_sensor[evsi_counter])\n",
    "    ################### lower branch ##############################################\n",
    "    dimension, x_train, x_test, x_cv = feature_remover(features_names = sensors)\n",
    "    \n",
    "    base_model = train_model(x_train, y_train, x_cv, y_cv, dimension['train_col'], 'base', model_path)\n",
    "    \n",
    "    base_prediction = base_model.predict_classes(x_test, verbose = 0)\n",
    "    \n",
    "    base_cost = get_expected_cost(base_prediction, ground_truth)\n",
    "    \n",
    "    ################### upper branch ##############################################\n",
    "    upper_dict = dict()\n",
    "    evsi_dict = dict()\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        # create a list of sensor that needs to get removed\n",
    "        remove_sensors = sensors.copy()\n",
    "        remove_sensors.remove(sensor)\n",
    "        \n",
    "        # prepare x \n",
    "        dimension, x_train, x_test, x_cv = feature_remover(features_names = remove_sensors)\n",
    "        \n",
    "        # train the model associated with current sensor, save the model, get the prediction\n",
    "        upper_model = train_model(x_train, y_train, x_cv, y_cv, dimension['train_col'], '+' + sensor, model_path)\n",
    "        upper_prediction = upper_model.predict_classes(x_test, verbose = 0)\n",
    "        \n",
    "        # calculate evsi\n",
    "        upper_dict[sensor] = get_expected_cost(upper_prediction, ground_truth)\n",
    "        evsi_dict[sensor] = base_cost - upper_dict[sensor]\n",
    "    \n",
    "    ################### rank and others ##############################################\n",
    "    # rank by evsi\n",
    "    ranked_sensors = sorted(evsi_dict.items(), key=lambda x: x[1], reverse= True)\n",
    "    sensor_to_monitor = ranked_sensors[0][0]\n",
    "    \n",
    "    # remove the top sensor from the list\n",
    "    sensors.remove(sensor_to_monitor)\n",
    "    \n",
    "    # updating some parameters\n",
    "    evsi_counter += 1\n",
    "    model_path = 'models/evsi/' + str(evsi_counter) + '_' + sensor_to_monitor + '/'\n",
    "    \n",
    "    # logging the evsi over the current sensors\n",
    "    with open('log/sensor_selection.csv', 'a', newline='') as out:\n",
    "        csv_out = csv.writer(out)\n",
    "        for row in ranked_sensors:\n",
    "            csv_out.writerow(row)\n",
    "        csv_out.writerow([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS_sensors = ['xmv_10', 'xmv_11', 'xmeas_19', 'xmeas_21', 'xmv_9', 'xmv_4', 'xmv_5', 'xmeas_17', 'xmeas_18', 'xmeas_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this for the next run\n",
    "unused_sensor = list()\n",
    "unused_sensor.append(['xmv_10', 'xmv_11', 'xmeas_19', 'xmeas_21', 'xmv_9', 'xmv_4', 'xmv_5', 'xmeas_17', 'xmeas_18', 'xmeas_9'])\n",
    "unused_sensor.append(['xmv_11', 'xmeas_19', 'xmeas_21', 'xmv_9', 'xmv_4', 'xmv_5', 'xmeas_17', 'xmeas_18', 'xmeas_9'])\n",
    "unused_sensor.append(['xmv_11', 'xmeas_19', 'xmeas_21', 'xmv_9', 'xmv_4', 'xmv_5', 'xmeas_17', 'xmeas_18'])\n",
    "unused_sensor.append(['xmv_11', 'xmeas_19', 'xmeas_21', 'xmv_9', 'xmv_4', 'xmeas_17', 'xmeas_18'])\n",
    "unused_sensor.append(['xmv_11', 'xmeas_19', 'xmv_9', 'xmv_4', 'xmeas_17', 'xmeas_18'])\n",
    "unused_sensor.append(['xmv_11', 'xmeas_19', 'xmv_9', 'xmv_4', 'xmeas_18'])\n",
    "unused_sensor.append(['xmv_11', 'xmeas_19', 'xmv_9', 'xmv_4'])\n",
    "unused_sensor.append(['xmeas_19', 'xmv_9', 'xmv_4'])\n",
    "unused_sensor.append(['xmv_9', 'xmv_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none: load\n",
      "xmv_10: load\n",
      "xmeas_21: train\n",
      "models/ml/2_xmeas_21/base\n",
      "44\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/base\\assets\n",
      "models/ml/2_xmeas_21/+xmv_11\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmv_11\\assets\n",
      "models/ml/2_xmeas_21/+xmeas_19\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmeas_19\\assets\n",
      "models/ml/2_xmeas_21/+xmv_9\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmv_9\\assets\n",
      "models/ml/2_xmeas_21/+xmv_4\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmv_4\\assets\n",
      "models/ml/2_xmeas_21/+xmv_5\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmv_5\\assets\n",
      "models/ml/2_xmeas_21/+xmeas_17\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmeas_17\\assets\n",
      "models/ml/2_xmeas_21/+xmeas_18\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmeas_18\\assets\n",
      "models/ml/2_xmeas_21/+xmeas_9\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/2_xmeas_21/+xmeas_9\\assets\n",
      "xmv_4: train\n",
      "models/ml/3_xmv_4/base\n",
      "45\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/base\\assets\n",
      "models/ml/3_xmv_4/+xmv_11\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/+xmv_11\\assets\n",
      "models/ml/3_xmv_4/+xmeas_19\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/+xmeas_19\\assets\n",
      "models/ml/3_xmv_4/+xmv_9\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/+xmv_9\\assets\n",
      "models/ml/3_xmv_4/+xmv_5\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/+xmv_5\\assets\n",
      "models/ml/3_xmv_4/+xmeas_17\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/+xmeas_17\\assets\n",
      "models/ml/3_xmv_4/+xmeas_18\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/+xmeas_18\\assets\n",
      "models/ml/3_xmv_4/+xmeas_9\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/3_xmv_4/+xmeas_9\\assets\n",
      "xmeas_18: train\n",
      "models/ml/4_xmeas_18/base\n",
      "46\n",
      "INFO:tensorflow:Assets written to: models/ml/4_xmeas_18/base\\assets\n",
      "models/ml/4_xmeas_18/+xmv_11\n",
      "47\n",
      "INFO:tensorflow:Assets written to: models/ml/4_xmeas_18/+xmv_11\\assets\n",
      "models/ml/4_xmeas_18/+xmeas_19\n",
      "47\n",
      "INFO:tensorflow:Assets written to: models/ml/4_xmeas_18/+xmeas_19\\assets\n",
      "models/ml/4_xmeas_18/+xmv_9\n",
      "47\n",
      "INFO:tensorflow:Assets written to: models/ml/4_xmeas_18/+xmv_9\\assets\n",
      "models/ml/4_xmeas_18/+xmv_5\n",
      "47\n",
      "INFO:tensorflow:Assets written to: models/ml/4_xmeas_18/+xmv_5\\assets\n",
      "models/ml/4_xmeas_18/+xmeas_17\n",
      "47\n",
      "INFO:tensorflow:Assets written to: models/ml/4_xmeas_18/+xmeas_17\\assets\n",
      "models/ml/4_xmeas_18/+xmeas_9\n",
      "47\n",
      "INFO:tensorflow:Assets written to: models/ml/4_xmeas_18/+xmeas_9\\assets\n",
      "xmeas_19: train\n",
      "models/ml/5_xmeas_19/base\n",
      "47\n",
      "INFO:tensorflow:Assets written to: models/ml/5_xmeas_19/base\\assets\n",
      "models/ml/5_xmeas_19/+xmv_11\n",
      "48\n",
      "INFO:tensorflow:Assets written to: models/ml/5_xmeas_19/+xmv_11\\assets\n",
      "models/ml/5_xmeas_19/+xmv_9\n",
      "48\n",
      "INFO:tensorflow:Assets written to: models/ml/5_xmeas_19/+xmv_9\\assets\n",
      "models/ml/5_xmeas_19/+xmv_5\n",
      "48\n",
      "INFO:tensorflow:Assets written to: models/ml/5_xmeas_19/+xmv_5\\assets\n",
      "models/ml/5_xmeas_19/+xmeas_17\n",
      "48\n",
      "INFO:tensorflow:Assets written to: models/ml/5_xmeas_19/+xmeas_17\\assets\n",
      "models/ml/5_xmeas_19/+xmeas_9\n",
      "48\n",
      "INFO:tensorflow:Assets written to: models/ml/5_xmeas_19/+xmeas_9\\assets\n",
      "xmv_9: train\n",
      "models/ml/6_xmv_9/base\n",
      "48\n",
      "INFO:tensorflow:Assets written to: models/ml/6_xmv_9/base\\assets\n",
      "models/ml/6_xmv_9/+xmv_11\n",
      "49\n",
      "INFO:tensorflow:Assets written to: models/ml/6_xmv_9/+xmv_11\\assets\n",
      "models/ml/6_xmv_9/+xmv_5\n",
      "49\n",
      "INFO:tensorflow:Assets written to: models/ml/6_xmv_9/+xmv_5\\assets\n",
      "models/ml/6_xmv_9/+xmeas_17\n",
      "49\n",
      "INFO:tensorflow:Assets written to: models/ml/6_xmv_9/+xmeas_17\\assets\n",
      "models/ml/6_xmv_9/+xmeas_9\n",
      "49\n",
      "INFO:tensorflow:Assets written to: models/ml/6_xmv_9/+xmeas_9\\assets\n",
      "xmeas_9: train\n",
      "models/ml/7_xmeas_9/base\n",
      "49\n",
      "INFO:tensorflow:Assets written to: models/ml/7_xmeas_9/base\\assets\n",
      "models/ml/7_xmeas_9/+xmv_11\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/ml/7_xmeas_9/+xmv_11\\assets\n",
      "models/ml/7_xmeas_9/+xmv_5\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/ml/7_xmeas_9/+xmv_5\\assets\n",
      "models/ml/7_xmeas_9/+xmeas_17\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/ml/7_xmeas_9/+xmeas_17\\assets\n",
      "xmv_5: train\n",
      "models/ml/8_xmv_5/base\n",
      "50\n",
      "INFO:tensorflow:Assets written to: models/ml/8_xmv_5/base\\assets\n",
      "models/ml/8_xmv_5/+xmv_11\n",
      "51\n",
      "INFO:tensorflow:Assets written to: models/ml/8_xmv_5/+xmv_11\\assets\n",
      "models/ml/8_xmv_5/+xmeas_17\n",
      "51\n",
      "INFO:tensorflow:Assets written to: models/ml/8_xmv_5/+xmeas_17\\assets\n"
     ]
    }
   ],
   "source": [
    "FS_counter = 0\n",
    "sensor_to_monitor = 'none'\n",
    "\n",
    "#write header\n",
    "with open('log/acc_improvement.csv', 'a', newline='') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['sensor', 'acc_improvement'])\n",
    "\n",
    "with open('log/acc.csv', 'a', newline='') as out:\n",
    "    csv_out = csv.writer(out)\n",
    "    csv_out.writerow(['sensor', 'acc'])\n",
    "    \n",
    "while len(FS_sensors) > 1:\n",
    "    ################### base accuracy ##############################################\n",
    "    dimension, x_train, x_test, x_cv = feature_remover(features_names = FS_sensors)\n",
    "    model_path = glob.glob('models/evsi/' + str(FS_counter) + '_*')[0] + '/'\n",
    "    \n",
    "    # evsi and fs match, just load the same model\n",
    "    if set(FS_sensors) == set(unused_sensor[FS_counter]):\n",
    "        base_model = load_model(model_path + 'base', compile = True)\n",
    "        print(sensor_to_monitor + ': load')\n",
    "        \n",
    "    # evsi and fs doesn't match, train the model\n",
    "    else:\n",
    "        model_path = 'models/ml/' + str(FS_counter) + '_' + sensor_to_monitor + '/'\n",
    "        print(sensor_to_monitor + ': train')\n",
    "        base_model = train_model(x_train, y_train, x_cv, y_cv, dimension['train_col'], 'base', model_path)\n",
    "        \n",
    "        \n",
    "    _, base_acc = base_model.evaluate(x_test, y_test, verbose = 0)\n",
    "    \n",
    "    ################### improved accuracy ##############################################\n",
    "    acc_dict = dict()\n",
    "    acc_improvement_dict = dict()\n",
    "    for sensor in FS_sensors:\n",
    "\n",
    "        # create a list of sensor that needs to get removed\n",
    "        remove_sensors = FS_sensors.copy()\n",
    "        remove_sensors.remove(sensor)\n",
    "\n",
    "        # prepare x \n",
    "        dimension, x_train, x_test, x_cv = feature_remover(features_names = remove_sensors)\n",
    "        \n",
    "        # evsi and fs match, just load the same model\n",
    "        if set(FS_sensors) == set(unused_sensor[FS_counter]):\n",
    "            improved_model = load_model(model_path + '+' + sensor, compile = True)\n",
    "        \n",
    "        # evsi and fs doesn't match, train the model\n",
    "        else:\n",
    "            improved_model = train_model(x_train, y_train, x_cv, y_cv, dimension['train_col'], '+' + sensor, model_path)\n",
    "            \n",
    "        _, acc_dict[sensor] = improved_model.evaluate(x_test, y_test, verbose = 0)\n",
    "        \n",
    "        \n",
    "        #calculate improvement\n",
    "        acc_improvement_dict[sensor] = acc_dict[sensor] - base_acc\n",
    "        \n",
    "    ################### rank and others ##############################################\n",
    "    \n",
    "    #rank by accuracy improvement\n",
    "    FS_ranked_sensors_by_improv = sorted(acc_improvement_dict.items(), key=lambda x: x[1], reverse= True)\n",
    "    sensor_to_monitor = FS_ranked_sensors_by_improv[0][0]\n",
    "\n",
    "    # remove the top sensor from the list\n",
    "    FS_sensors.remove(sensor_to_monitor)\n",
    "\n",
    "    # updating some parameters\n",
    "    FS_counter += 1\n",
    "    model_path = 'models/ml/' + str(FS_counter) + '_' + sensor_to_monitor + '/'\n",
    "\n",
    "    # logging the improvement over the current sensors\n",
    "    with open('log/acc_improvement.csv', 'a', newline='') as out:\n",
    "        csv_out = csv.writer(out)\n",
    "        for row in FS_ranked_sensors_by_improv:\n",
    "            csv_out.writerow(row)\n",
    "        csv_out.writerow([])\n",
    "    \n",
    "    \n",
    "    # logging the accuracy over the current sensors\n",
    "    FS_ranked_sensors_by_acc = sorted(acc_dict.items(), key=lambda x: x[1], reverse= True)\n",
    "    with open('log/acc.csv', 'a', newline='') as out:\n",
    "        csv_out = csv.writer(out)\n",
    "        for row in FS_ranked_sensors_by_acc:\n",
    "            csv_out.writerow(row)\n",
    "        csv_out.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
